---
categories:  
- ""    #the front matter should be like the one found in, e.g., blog2.md. It cannot be like the normal Rmd we used
- ""
date: "2021-09-30"
description: London Business School last group homework # the title that will show up once someone gets to this page
draft: false
image: London.jpg # save picture in \static\img\blogs. Acceptable formats= jpg, jpeg, or png . Your iPhone pics wont work

keywords: ""
slug: lbs # slug is the shorthand URL address... no spaces plz
title: London Business School last group homework
---

```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```


```{r load-libraries, echo=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(GGally)
library(readxl)
library(here)
library(skimr)
library(janitor)
library(broom)
library(tidyquant)
library(infer)
library(openintro)
```


> Introduction : In our assignement, we will calculate statistical summaries of our variables and test the confidence level of the overall model. The p-value associated to the t-stat, (the Student test) tests whether each independent variable of the null hypothesis has a correlation with the dependent variable. The lower the p-value, the greater the statistical significance of the observed difference. The smaller the p-value, the higher the chance there is a correlation between the changes in the independent variable and the shifts in the dependent variable. As a result, you can reject H0 that states there is no correlation between the variables at a (1-alpha) (usually 95%) confidence level. Nonetheless, you cannot say for sure that H1 is true. You can only estimate the confidence level of your data. 

# Youth Risk Behavior Surveillance

Every two years, the Centers for Disease Control and Prevention conduct the [Youth Risk Behavior Surveillance System (YRBSS)](https://www.cdc.gov/healthyyouth/data/yrbs/index.htm) survey, where it takes data from high schoolers (9th through 12th grade), to analyze health patterns. You will work with a selected group of variables from a random sample of observations during one of the years the YRBSS was conducted.

## Load the data

This data is part of the `openintro` textbook and we can load and inspect it. There are observations on 13 different variables, some categorical and some numerical. The meaning of each variable can be found by bringing up the help file:

```{r}
data(yrbss)
glimpse(yrbss)
```

Before you carry on with your analysis, it's is always a good idea to check with `skimr::skim()` to get a feel for missing values, summary statistics of numerical variables, and a very rough histogram.
```{r}
skim(yrbss)
```

## Exploratory Data Analysis

You will first start with analyzing the `weight` of participants in kilograms. Using visualization and summary statistics, describe the distribution of weights. How many observations are we missing weights from?

> From the glimpse function, we can see that the weight column has 1004 missing values with the complete rate of 92.6%. THe mean of the weight is at 67.9 kg and the median is at 56.2 kg. The weight data is also skewed to the right as the median is less than mean. We will demonstrate the visualization of this data in the graph below. 

```{r, eda_on_weight}

# show the important statistic 
favstats(~weight, data=yrbss)
```
```{r}
#Create the ggplot for boxplot
ggplot(yrbss, aes (x=weight))+
  geom_boxplot()+
  theme_minimal()
  NULL
```
> From the box plot we can see that there are many outliers in the dataset. 

```{r}
#Create the ggplot for Distribution
ggplot(yrbss, aes (x=weight))+
  theme_minimal()+ 
  geom_density(alpha = 0.3) +
  labs()
  NULL
```
> From the graph you can see that the data is positive skew as the mean is higher than the median.


```{r}
# Create the t statistic test 
t.test(~ weight, data=yrbss)
```


> Team comment: the p-value is very small which means we can reject H0: mean = 0. The distribution of weights does not follow an exact standard normal distribution. However if we tried to do the summary of the weight centered-reduced we have the following result: 

```{r}
weight_1 <- (yrbss$weight - 67.9)/16.9
t.test(~ weight_1, data=yrbss)
ggplot(yrbss, aes (x=weight_1))+
   geom_density(alpha = 0.3)
```
> The p-value equals one, which is the expected result as we centered and normalized our distribution.

Next, consider the possible relationship between a high schooler’s weight and their physical activity. Plotting the data is a useful first step because it helps us quickly visualize trends, identify strong associations, and develop research questions.

Let’s create a new variable in the dataframe `yrbss`, called `physical_3plus` , which will be `yes` if they are physically active for at least 3 days a week, and `no` otherwise. You may also want to calculate the number and % of those who are and are not active for more than 3 days. Use the `count()` function and see if you get the same results as `group_by()... summarise()`

  
```{r, mutate_and_count}
yrbss_mutate <- yrbss %>%
  
  # Mutate column to create yes and no depend on whether the student exercises at least 3 days or not 
  
  mutate(physical_3plus = ifelse(physically_active_7d >=3, "yes","no")) %>% 
  
# mutate this column to calculate proportion, 1 mean that people exercise more than or equal to 3 days and 0 mean they exercise less than 3 days
  
   mutate(physical_3plusn = ifelse(physically_active_7d >=3, 1,0)) %>% 
na.omit()

yrbss_count <- yrbss_mutate %>%  
  
  #Count the number of yes and no
  count(physical_3plus, name = "count") %>%
  
  # Omit the n/a value in the dataframe
  na.omit()

# Create the ybrss_proportion to create the percentage proportion of of people who exercise less than 3 days and proportion of people who exercise more than 3 days.

ybrss_proportion <- yrbss_count %>% 
  mutate(percentage_no =yrbss_count[1,2]/(yrbss_count[1,2]+yrbss_count[2,2]),
percentage_yes= yrbss_count[2,2]/(yrbss_count[1,2]+yrbss_count[2,2]))

# Create table name and column name 
knitr::kable(ybrss_proportion, caption = "Table Summary for proportion of student excercise at least 3 days or less than 3 days", col.names = c("Excercise at least 3 days", "No of sample", "Proportion excercise less than 3 day","Proportion excercise at least 3day"), digit=3)

```




```{r}
#Count the number of yes and no with a different method group_by()... summarize()
yrbss_m <- yrbss %>% 
 na.omit() %>% 
  group_by(physical_3plus = ifelse(physically_active_7d >=3, "yes","no")) %>%
  summarise(count = n())

 # Create table name and column name 
knitr::kable(yrbss_m, caption = "Proportion of student excercise at least 3 days or less than 3 days", col.names = c("Excercise at least 3 days", "No of sample"))

```



Can you provide a 95% confidence interval for the population proportion of high schools that are *NOT* active 3 or more days per week?

```{r}
ybrss_table <- yrbss_mutate %>%
  #Calculate the 95% confidence interval
  #First we summarize the proportion of people who exercise less than 3 days
  # Proportion of people lower than 3 days need to minus 1 
  summarize(proportion_lower3days = 1-mean(physical_3plusn), 
            count = n(),
            t_critical= qt(0.975,count-1),
  # In this case the se error formula is difference from the se in the mean 
            se=sqrt((proportion_lower3days*(1-proportion_lower3days)/count)),
            margin_of_error= t_critical * se,
            low95=proportion_lower3days-margin_of_error,
            high95=proportion_lower3days+margin_of_error)

 # Create table name and column name 
knitr::kable(ybrss_table, caption = "Summary Statistic table", col.names = c("Proportion Excercise less than 3 days", "No of sample","T_critical","SE","Margin of Error","low95","high95"))



```

```{r}
ggplot(ybrss_table)+
  # plot the point for the proportion of of people who exercise less than 3 days 
  geom_point(aes(x=proportion_lower3days,y=count))+
  geom_vline(xintercept = 0.308)+
  geom_vline(xintercept = 0.328)+ 
  xlim(0.25, 0.4)+
  theme(axis.text.y = element_blank())+ 
  theme(axis.text.y = element_blank())+
  labs(title="95% Confidence interval for proportion of people who excercise less than 3 days",
       x="Proportion of people excercise less than 3 days",
       y="")
  

```

  
Make a boxplot of `physical_3plus` vs. `weight`. Is there a relationship between these two variables? What did you expect and why?
```{r}
# Generate the dataframe with column with yes and no for hte people who exercise more than or equal to 3 days
yrbss_mutate1 <- yrbss %>%
  
  # Adding one column physical_3plus to the dataframe 1 mean that people exercise more than or equal to 3 days and 0 mean they exercise less than 3 days
  
  mutate(physical_3plus = ifelse(physically_active_7d >=3, "Yes","No")) %>% 
na.omit()
```

```{r}

yrbss_mutate1 %>%
  na.omit() %>%
  ggplot(aes (x=physical_3plus, y = weight))+
  geom_boxplot()+
  theme_minimal()+ 
  labs(title = "Weight Comparison", x="Do you work out at least 3 day per week", y = "Weight") +
  NULL
```

From this chart we can see a strange pattern that people who work out more than 3 days a week has higher weight than people who does not.Therefore, we need to remove the gender bias by creating the box plot for men and women. 



```{r, boxplot}
yrbss_mutate1 %>%
  filter(gender == "male") %>%
  na.omit() %>%
  ggplot(aes (x=physical_3plus, y = weight))+
  geom_boxplot()+
  theme_minimal()+ 
  labs(title = "Male Weight Comparison", x="Do you work out at least per week", y = "Weight") +
  NULL
```


```{r}
yrbss_mutate1 %>%
  filter(gender == "female") %>%
  na.omit() %>%
  ggplot(aes (x=physical_3plus, y = weight))+
  geom_boxplot()+
  theme_minimal()+ labs(title = "Female Weight Comparison", x="Do you work out more than 3 day per week", y = "Weight") +
  NULL
```
> We decided to filter by gender. The weights are different according to the gender, so we decided to separate women and men to be able to draw an accurate conclusion. As the males tend to have more weight and muscle mass than female and they tend to work out at least 3 days per week.  

> For males, the interesting result is that median weights are pretty similar whether you work out or not. It is probably because males who work out gain muscle so gain weight. For females however, we see the median weight is slightly below for those who work out more than 3 days per week.  

## Confidence Interval

Boxplots show how the medians of the two distributions compare, but we can also compare the means of the distributions using either a confidence interval or a hypothesis test. Note that when we calculate the mean, SD, etc. weight in these groups using the mean function, we must ignore any missing values by setting the `na.rm = TRUE`.


```{r, ci_using_formulas}
yes_ci <- yrbss_mutate1 %>%
  filter(physical_3plus == "Yes") %>%
summarize(average = mean(weight,na.rm=TRUE), 
            sd = sd(weight,na.rm=TRUE), 
            count = n(),
            t_critical= qt(0.975,count-1),
            se=sd/sqrt(count),
            margin_of_error= t_critical * se,
            low95=average-margin_of_error,
            high95=average+margin_of_error)
yes_ci



no_ci <- yrbss_mutate1 %>%
  filter(physical_3plus == "No") %>%
summarize(average = mean(weight,na.rm=TRUE), 
            sd = sd(weight,na.rm=TRUE), 
            count = n(),
            t_critical= qt(0.975,count-1),
            se=sd/sqrt(count),
            margin_of_error= t_critical * se,
            low95=average-margin_of_error,
            high95=average+margin_of_error)
no_ci
```

There is an observed difference of about 1.6kg (68.7 - 67.1 kg), and we notice that the two confidence intervals do not overlap. It seems that the difference is at least 95% statistically significant. Let us also conduct a hypothesis test.

## Hypothesis test with formula

Write the null and alternative hypotheses for testing whether mean weights are different for those who exercise at least times a week and those who don’t.

```{r, t_test_using_R}
 t.test(weight ~ physical_3plus, data = yrbss_mutate1)
```


## Hypothesis test with `infer`


Next, we will introduce a new function, `hypothesize`, that falls into the infer workflow. You will use this method for conducting hypothesis tests.

But first, we need to initialize the test, which we will save as `obs_diff`.

```{r, calc_obs_difference}
obs_diff <- yrbss_mutate1 %>%
  specify(weight ~ physical_3plus) %>%
  calculate(stat = "diff in means", order = c("Yes", "No"))
obs_diff
```



Notice how you can use the functions specify and calculate again like you did for calculating confidence intervals. Here, though, the statistic you are searching for is the difference in means, with the order being yes - no != 0.

After you have initialized the test, you need to simulate the test on the null distribution, which we will save as null.


```{r, hypothesis_testing_using_infer_package}

null_dist <- yrbss_mutate1 %>%
  # specify variables
  specify(weight ~ physical_3plus) %>%
  
  # assume independence, i.e, there is no difference
  hypothesize(null = "independence") %>%
  
  # generate 1000 reps, of type "permute"
  generate(reps = 1000, type = "permute") %>%
  
  # calculate statistic of difference, namely "diff in means"
  calculate(stat = "diff in means", order = c("Yes", "No"))
null_dist
```

Here, `hypothesize` is used to set the null hypothesis as a test for independence, i.e., that there is no difference between the two population means. In one sample cases, the null argument can be set to *point* to test a hypothesis relative to a point estimate.

Also, note that the `type` argument within generate is set to permute, which is the argument when generating a null distribution for a hypothesis test.

We can visualize this null distribution with the following code:

```{r}
ggplot(data = null_dist, aes(x = stat)) +
  geom_histogram()

```

Now that the test is initialized and the null distribution formed, we can visualise to see how many of these null permutations have a difference of at least `obs_stat` of `r obs_diff %>% pull() %>% round(2)`?

We can also calculate the p-value for your hypothesis test using the function `infer::get_p_value()`.

```{r}

null_dist %>% visualize() +
  shade_p_value(obs_stat = obs_diff, direction = "two-sided")

null_dist %>%
  get_p_value(obs_stat = obs_diff, direction = "two_sided")

```
This the standard workflow for performing hypothesis tests.

# IMDB ratings: Differences between directors

Recall the IMBD ratings data. I would like you to explore whether the mean IMDB rating for Steven Spielberg and Tim Burton are the same or not. I have already calculated the confidence intervals for the mean ratings of these two directors and as you can see they overlap.

```{r directors, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "directors.png"), error = FALSE)
```

First, I would like you to reproduce this graph. You may find `geom_errorbar()` and `geom_rect()` useful.

In addition, you will run a hypothesis test. You should use both the `t.test` command and the `infer` package to simulate from a null distribution, where you assume zero difference between the two.

> Before anything, write down the null and alternative hypotheses, as well as the resulting test statistic and the associated t-stat or p-value. At the end of the day, what do you conclude?

[**Hypothesis:**]{.ul}

$H_0$ : $mean(rating^{Spielberg})$ = $mean(rating^{Burton})$

$H_A$ : $mean(rating^{Spielberg})$ != $mean(rating^{Burton})$

-   Null Hypothesis: mean rating of Steven Spielberg's movies is the same as that of Tim Burton's

-   Alternative Hypothesis: mean rating of Steven Spielberg's movies is not the same as that of Tim Burton's

[**Statistics:**]{.ul}

-   testing statistics: $\frac{mean(rating^S)-mean(rating^B)}{\sqrt{\frac{sd^S}{n^S}+\frac{sd^B}{n^B}}}$

-   $t = 3$

-   $p-value = 0.01$

[**Conclusions:**]{.ul}

-   We should reject the null hypothesis on 95% confidence interval. Although the confidence intervals of the two groups overlap, a two sample t-test has proved that Steven Spielberg's movies have statistically significantly higher mean ratings than Tim Burton's movies.

You can load the data and examine its structure

```{r load-movies-data}
movies <- read_csv(here::here("data", "movies.csv"))
glimpse(movies)
```

Your R code and analysis should go here. If you want to insert a blank chunk of R code you can just hit `Ctrl/Cmd+Alt+I`

```{r, reproduce_graph, message = FALSE}
movies_p <- movies %>% 
  filter(director %in% c("Tim Burton", "Steven Spielberg")) %>% 
  group_by(director) %>% 
  summarise(mean_rating = mean(rating), 
            sd_rating = sd(rating), 
            sqrt_n = n()**0.5,
            t_critical = qt(0.975, n()-1),
            se = sd_rating/sqrt_n,
            upper = mean_rating + t_critical*se,
            lower = mean_rating - t_critical*se)

ggplot(movies_p) +
  geom_point(aes(x = director, y = mean_rating, color = director), position = position_dodge(0.1), size = 8) +
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = 7.27,ymax = 7.33), fill = "grey80", alpha = 0.5)+
  geom_errorbar(aes(x = director, y = mean_rating, ymin = lower, ymax = upper, color = director), 
                width = 0.05, size = 3, position = position_dodge(0.9))+
  geom_text(aes(x = director, y = mean_rating, label = round(mean_rating, digits = 2)), vjust=-2, size = 8)+
  geom_point(aes(x = director, y = upper, color = director), position = position_dodge(0.1), size = 1) +
  geom_text(aes(x = director, y = upper, label = round(upper, digits = 2)),vjust=-2, size = 5)+
  geom_point(aes(x = director, y = lower, color = director), position = position_dodge(0.1), size = 1) +
  geom_text(aes(x = director, y = lower, label = round(lower, digits = 2)),vjust=-2, size = 5)+
  coord_flip()+
  labs(title = "Do Spielberg and Burton have the same IMDB ratings?",
       subtitle = "95% confidence interval overlap",
       x = "",
       y = "Mean IMDB Rating") +
  theme_bw() +
  theme(legend.position = "none",
        title=element_text(size=16, face="bold"))

```

```{r, hypothesis_test_t}
movies_t <- movies %>% 
  filter(director %in% c("Steven Spielberg", "Tim Burton"))

t.test(rating ~ director, data = movies_t)
```

## Hypothesis test with `infer`

```{r, obs_difference}
obs_diff <- movies_t %>%
  specify(rating ~ director) %>%
  calculate(stat = "diff in means", order = c("Steven Spielberg", "Tim Burton"))
```

```{r, hypothesis_testing_using_infer}

null_dist <- movies_t %>%
  # specify variables
  specify(rating ~ director) %>%
  
  # assume independence, i.e, there is no difference
  hypothesize(null = "independence") %>%
  
  # generate 1000 reps, of type "permute"
  generate(reps = 1000, type = "permute") %>%
  
  # calculate statistic of difference, namely "diff in means"
  calculate(stat = "diff in means", order = c("Steven Spielberg", "Tim Burton"))

```

```{r, visualizing_null_hypothesis}
ggplot(data = null_dist, aes(x = stat)) +
  geom_histogram()

```

```{r}

null_dist %>% visualize() +
  shade_p_value(obs_stat = obs_diff, direction = "two-sided")

null_dist %>%
  get_p_value(obs_stat = obs_diff, direction = "two_sided")

```


# Omega Group plc- Pay Discrimination


At the last board meeting of Omega Group Plc., the headquarters of a large multinational company, the issue was raised that women were being discriminated in the company, in the sense that the salaries were not the same for male and female executives. A quick analysis of a sample of 50 employees (of which 24 men and 26 women) revealed that the average salary for men was about 8,700 higher than for women. This seemed like a considerable difference, so it was decided that a further analysis of the company salaries was warranted. 

You are asked to carry out the analysis. The objective s to find out whether there is indeed a significant difference between the salaries of men and women, and whether the difference is due to discrimination or whether it is based on another, possibly valid, determining factor. 

## Loading the data


```{r load_omega_data}
omega <- read_csv(here::here("data", "omega.csv"))
glimpse(omega) # examine the data frame
skim(omega) # examine the data frame 
```

## Relationship Salary - Gender ?

The data frame `omega`  contains the salaries for the sample of 50 executives in the company. Can you conclude that there is a significant difference between the salaries of the male and female executives?

Note that you can perform different types of analyses, and check whether they all lead to the same conclusion 

.	Confidence intervals
.	Hypothesis testing
.	Correlation analysis
.	Regression


Calculate summary statistics on salary by gender. Also, create and print a dataframe where, for each gender, you show the mean, SD, sample size, the t-critical, the SE, the margin of error, and the low/high endpoints of a 95% confidence interval


```{r}
# Summary Statistics of salary by gender
mosaic::favstats (salary ~ gender, data=omega)

omega_table <-omega %>% 
  
  #Group by genre in the dataframe
  group_by(gender) %>% 
  
  # Use summarize the create the new statistic column
    summarise(mean_salary= mean(salary,na.rm=TRUE),
            sd_salary=sd(salary,na.rm=TRUE),
            count=n(),
            
            # get 95% t critical value
            t_critical= qt(0.975,count-1), 
            se_salary= sd_salary/sqrt(count),
            margin_of_error= t_critical * se_salary,
            salary_low95= mean_salary-margin_of_error,
            salary_high95=mean_salary+margin_of_error)

# Create a table using kable and label title and column
knitr::kable(omega_table, caption = "Summary Statistic of salaries from 50 executives by gender", 
             col.names = c("Gender", "Mean salary", "SD Salary" , "Number of samples","t-critical", "SE Salary", "Margin of error", "Salary Low 95%", "Salary High 95%"))
  

```
We will plot the confidence interval of both gender to see the if it overlap with each other 

```{r}
ggplot(data=omega_table,aes(x=mean_salary,y=gender))+
  
  #plot the mean as a point in the graph 
  geom_point()+
  
  #Create the confidence interval in the graph
  geom_errorbar(aes(xmin=salary_low95, xmax=salary_high95))+
  
  #Label the axis and title
  labs(title="95% confidence interval for salary mean",
       x= "Average Salary",
       y="Gender")+
  
  # Add theme to the graph 
  theme_bw()
```

> What can you conclude from your analysis? A couple of sentences would be enough

> We have create the 95% confidence interval from the data by gender and then we graphed the result from the table to provide visualiztion. From the graph we can see the two confidence intervals do not overlap each other. The male CI is between 70,088 and 76,390 and female is between 61,486 to 67,599. So we can tell from the graph that there is a significant difference between the mean of salary between men and women (95% confidence Interval)

You can also run a hypothesis testing, assuming as a null hypothesis that the mean difference in salaries is zero, or that, on average, men and women make the same amount of money. You should tun your hypothesis testing using `t.test()` and with the simulation method from the `infer` package.

First we would like to write the null and alternative hypothesis. (this is the two tail test)

$H_0$ : $meanmen$ = $meanwomen$
Null Hypothesis: observed mean of men salary is the same as mean of women salary 

$H_A$ : $meanmen$ =! $meanwomen$         
Alternative Hypothesis: observed mean of men salary is not the same as mean of women salary 

Then we create the t test statistic using code in R
```{r}
# hypothesis testing using t.test() 

# Change the scientific number into normal number
options(scipen = 1)
t.test(salary ~ gender, data=omega)


```

```{r}
# hypothesis testing using infer package

#Initialized the test 
obs_diffomega <- omega %>%
  specify(salary~ gender) %>%
  calculate(stat = "diff in means", order = c("male", "female"))


# Set the origin of the sample data 
set.seed(1234)

 omega_infer <- omega %>% 

#Specify the variable of interest (mean and gender)
  specify(salary~ gender) %>% 
   
#Hypothesize a null of no of difference 
   hypothesize(null="independence") %>% 
   
#Generate a bunch of bootstrap sample 
  generate(reps=1000,type="permute") %>% 
   
# calculate the mean of each sample
  calculate (stat="diff in means",
             order= c("male","female"))
 
 omega_infer %>% 
   visualize
 
 #Calculate the P Value 
 omega_infer %>% 
   get_p_value(obs_stat = obs_diffomega,direction = "two-sided")



```

```{r}
# Visuzlize the data using histogram
ggplot(data = omega_infer, aes(x = stat)) +
  geom_histogram()

```
```{r}
# Create the P value shade 
omega_infer %>% visualize() +
  shade_p_value(obs_stat = obs_diffomega, direction = "two-sided")

omega_infer %>%
  get_p_value(obs_stat = obs_diffomega, direction = "two_sided")
```


> What can you conclude from your analysis? A couple of sentences would be enough

> We reject the null hypothesis so in this case there is a statistically difference in the salary of men and women. 



## Relationship Experience - Gender?

At the board meeting, someone raised the issue that there was indeed a substantial difference between male and female salaries, but that this was attributable to other reasons such as differences in experience. A questionnaire send out to the 50 executives in the sample reveals that the average experience of the men is approximately 21 years, whereas the women only have about 7 years experience on average (see table below).

```{r, experience_stats}
# Summary Statistics of salary by gender
favstats (experience ~ gender, data=omega)

```

Based on this evidence, can you conclude that there is a significant difference between the experience of the male and female executives? Perform similar analyses as in the previous section. Does your conclusion validate or endanger your conclusion about the difference in male and female salaries?  

We will start by creating the summary table for important statistic for the experience between male and female
```{r, confint_single_valiables}
# Summary Statistics of salary by gender

omega_experience <-omega %>% 
  
  #Group by genre in the dataframe
  group_by(gender) %>% 
  
  # Use summarize the create the new statistic column
    summarise(mean_experience= mean(experience,na.rm=TRUE),
            sd_experience=sd(experience,na.rm=TRUE),
            count=n(),
            
            # get 95% t critical value
            t_critical= qt(0.975,count-1), 
            se_experience= sd_experience/sqrt(count),
            margin_of_error= t_critical * se_experience,
            experience_low95= mean_experience-margin_of_error,
            experience_high95=mean_experience+margin_of_error)

# Create a table using kable and label title and column
knitr::kable(omega_experience, caption = "Summary Statistic of working experience from 50 executives by gender", 
             col.names = c("Gender", "Mean Experience", "SD Experience" , "Number of samples","t-critical", "SE Experience", "Margin of error", "Salary Low 95%", "Salary High 95%"))
  

```

Graph the data to visualize the 95% confidence interval between men and women
```{r}
ggplot(data=omega_experience,aes(x=mean_experience,y=gender))+
  
  #plot the mean as a point in the graph 
  geom_point()+
  
  #Create the confidence interval in the graph
  geom_errorbar(aes(xmin=experience_low95, xmax=experience_high95))+
  
  #Label the axis and title
  labs(title="95% confidence interval for experience mean",
       x= "Average Experience",
       y="Gender")+
  
  # Add theme to the graph 
  theme_bw()
```

> From the graph we create the range of the 95% confidence interval for the experience between men and women and we see that the confidence interval does not overlap each other. Male is between 16.52 to 25.7 years while female is between 3.95 and 10.8 years. This means that there is a statiscal diferrence between men and womens work experience. 

First we would like to write the null and alternative hypothesis. (this is the two tail test)

$H_0$ : $meanexperiencemen$ = $meanexperiencewomen$
Null Hypothesis: observed mean of men experience is the same as mean of women experience 

$H_A$ : $meanexperiencemen$ =! $meanexperiencewomen$         
Alternative Hypothesis: observed mean of men experience is not the same as mean of women experience 

Then we create the t test statistic using code in R
```{r}
# hypothesis testing using t.test() 

options(scipen = 2) 
t.test(experience ~ gender, data=omega)

```

```{r, hypothesis_testing}
# hypothesis testing using infer package

#Initialized the test 
obs_diffexperience <- omega %>%
  specify(experience~ gender) %>%
  calculate(stat = "diff in means", order = c("male", "female"))

# Set the origin of the sample data 
set.seed(1234)


 omega_infer1 <- omega %>% 

#Specify the variable of interest (mean and gender)
  specify(experience~ gender) %>% 
   
#Hypothesize a null of no of difference 
   hypothesize(null="independence") %>% 
   
#Generate a bunch of bootstrap sample 
  generate(reps=1000,type="permute") %>% 
   
# calculate the mean of each sample
  calculate (stat="diff in means",
             order= c("male","female"))
 
  #Calculate the P Value 
 omega_infer1 %>% 
   get_p_value(obs_stat = obs_diffexperience ,direction = "two-sided")

 



```

```{r}
# Visuzlize the data using histogram
ggplot(data = omega_infer1, aes(x = stat)) +
  geom_histogram()

```


```{r}
# Create the P value shade 
omega_infer1 %>% visualize() +
  shade_p_value(obs_stat = obs_diffexperience, direction = "both")

omega_infer1 %>%
  get_p_value(obs_stat = obs_diffexperience, direction = "both")
```
## Relationship Salary - Experience ?

Someone at the meeting argues that clearly, a more thorough analysis of the relationship between salary and experience is required before any conclusion can be drawn about whether there is any gender-based salary discrimination in the company.

Analyse the relationship between salary and experience. Draw a scatterplot to visually inspect the data



```{r, salary_exp_scatter}
library(ggpubr)

ggplot(data=omega,aes(x=experience,y=salary))+
  geom_point()+
  geom_smooth(formula = y ~ x,method='lm',se=FALSE)+
  labs(title="Relationship between salary and experience",x="Experience",y="Salary")+
  theme_bw()

```
> From the graph we can see that there is a positive relationship between experience and salary meaning that people with more experience will have a higher salary. 

## Check correlations between the data
You can use `GGally:ggpairs()` to create a scatterplot and correlation matrix. Essentially, we change the order our variables will appear in and have the dependent variable (Y), salary, as last in our list. We then pipe the dataframe to `ggpairs()` with `aes` arguments to colour by `gender` and make ths plots somewhat transparent (`alpha  = 0.3`).

```{r, ggpairs}
omega %>% 
  select(gender, experience, salary) %>% #order variables they will appear in ggpairs()
  ggpairs(aes(colour=gender, alpha = 0.3))+
  theme_bw()
```

> Look at the salary vs experience scatterplot. What can you infer from this plot? Explain in a couple of sentences

>From the salary vs experience scatterplot it plots the salary in y and experience in x and it shows the dot in colour base on gender (men=green, women=red). From what we can see that there is a positive relationship between salary and experience and if we look closely, the majority of men have more experience than women; therefore, the experience may be one of the factors that results in men having higher salaries than women. So there is a correlation between gender and salary but it is not a causation as the experience is the causation of the salary. 



```{r Regression}

#assign number on the dummy variable
omega$gender1 <- as.numeric(factor(omega$gender))-1 #0: female, 1: male

#regress salary on experience and see its significance
# salary = a + b1 * experience + standard error1
model1<- lm(salary ~ experience, data= omega)
summary(model1)

# salary = a + b1 * experience + b2 * gender + standard error2
#regress salary on experience and see its significance
model2<- lm(salary ~ experience + gender1, data= omega)
summary(model2)

```

> Whether or not whether gender dummy variables are considered,both regressions show that experience variable has a significant correlations ot the salary

> Regression of model 2 can also show that the coefficient of gender dummy variable is insignificant. Thus, the regression 2 shows that between the gender and experience, the experience is the only significant predictor for one's salary.

# Challenge 1: Yield Curve inversion

Every so often, we hear warnings from commentators on the "inverted yield curve" and its predictive power with respect to recessions. An explainer what a [inverted yield curve is can be found here](https://www.reuters.com/article/us-usa-economy-yieldcurve-explainer/explainer-what-is-an-inverted-yield-curve-idUSKBN1O50GA). If you'd rather listen to something, here is a great podcast from [NPR on yield curve indicators](https://www.podbean.com/media/share/dir-4zgj9-6aefd11)

In addition, many articles and commentators think that, e.g., [*Yield curve inversion is viewed as a harbinger of recession*](https://www.bloomberg.com/news/articles/2019-08-14/u-k-yield-curve-inverts-for-first-time-since-financial-crisis). One can always doubt whether inversions are truly a harbinger of recessions, and [use the attached parable on yield curve inversions](https://twitter.com/5_min_macro/status/1161627360946511873).


```{r yield_curve_parable.jpg, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "yield_curve_parable.jpg"), error = FALSE)
```
> This very funny test illustrates completely the spirit of financial markets and self-fulfilling prophecies. The current potential inflation risk could actually build up due to self-fulfilling prophecies. For example today, we see rising energy prices with the recent surge in gas prices. If the credibility of central banks was to be lost, people will buy commodities right now despite high prices because they will fear that prices will increase even more - in reality, investors would be the ones to exert even more upward pressure on prices and create inflation. 

In our case we will look at US data and use the [FRED database](https://fred.stlouisfed.org/) to download historical yield curve rates, and plot the yield curves since 1999 to see when the yield curves flatten. If you want to know more, a very nice article that explains the [yield curve is and its inversion can be found here](https://fredblog.stlouisfed.org/2018/10/the-data-behind-the-fear-of-yield-curve-inversions/). At the end of this challenge you should produce this chart

```{r yield_curve_challenge, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "yield_curve_challenge.png"), error = FALSE)
```


First, we will load the yield curve data file that contains data on the yield curve since 1960-01-01

```{r download_historical_yield_curve, warning=FALSE}

yield_curve <- read_csv(here::here("data", "yield_curve.csv"))

glimpse(yield_curve)
```

Our dataframe `yield_curve` has five columns (variables):

- `date`: already a date object
- `series_id`: the FRED database ticker symbol
- `value`: the actual yield on that date
- `maturity`: a short hand for the maturity of the bond
- `duration`: the duration, written out in all its glory!


## Plotting the yield curve

This may seem long but it should be easy to produce the following three plots

### Yields on US rates by duration since 1960

```{r yield_curve_1, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "yield_curve1.png"), error = FALSE)
```
```{r}
yield_curve %>%
  ggplot(aes(x = date, y = value, colour = duration)) + 
  facet_wrap (~duration, nrow= 6) + geom_line() +
  labs(title="Yields on U.S. Treasury rates since 1960",
       x="",
       caption="Source: St. Louis Federal Reserve Economic Database (FRED)")+
  theme(legend.position = "none")

```
> In the recent years, yields decreased a lot. There are two contradictory forces that determine yield movements. When there is a crisis, or when the overall macroeconomic situation is negative, central banks tend to intervene to exert downard pressure on yields. However, if a specific country or company is not doing well, and decrease its overall credit quality, yields will increase.
> In the US, yields decreased a lot over the years, and offered great support to equities. In Europe, interest rates are even lower and became negative in 2019, before the Covid Crisis. The 10-year German Yield is currently around -0.3% and supposedly “high” yields countries such as Greece and Italy (with Debt-to-GDP ratios reaching 200 % and 160% respectively) post 10-year yields around 0.8% (well below the level they reached 5 years ago)! Countries such as Greece and Italy should have higher yields as they post high Debt-to-GDP ratio. But the overall macroeconomic situation with very low rates exerts too much downward pressure which means the fundamentals are gone. According to the Financial Times,  “One of the big dangers is that the market has become complacent about the relationship between interest rates and credit spreads.” Spreads are everywhere tight. 
> Overall, we see that yields are beguinning to rise again. However, the inflation threat is more likely to build up in the US rather than in Europe. In Europe due to multiple issues such as excess saving supply in Germany, ageing population, stagnation with low growth for a long time now, rates should stay low.

### Monthly yields on US rates by duration since 1999 on a year-by-year basis


```{r yield_curve_2, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "yield_curve2.png"), error = FALSE)
```
```{r}
monthly_duration <- yield_curve %>%
  mutate(year = year(date)) %>%
  filter (year > 1998) %>%
  mutate (month = month(date)) 

library(forcats)

monthly_duration$maturity <- fct_relevel(monthly_duration$maturity, c("3m", "6m", "1y", "2y","3y","5y", "7y","10y","20y","30y" ))

levels(monthly_duration$maturity)
  ggplot(data= monthly_duration, mapping = aes(x=maturity, y=value, colour = year, group = month)) + 
    geom_line() + 
    facet_wrap (~year, nrow= 6) + 
    labs(title = "US Yield Curve", x= "Maturity", y ="Yield (%)",caption= "Source: St. Louis Federal Reserve Economic Database (FRED)")+
    theme_bw() + 
    theme(legend.position = "none") + 
    theme(axis.text.y =element_text(size=5))+ 
    theme(axis.text.x =element_text(size=5))+
    NULL
 
```

> What is very interesting here, is to see how the US yield curve decreased a lot and became flat. In 1999 and 2000, yields were very high around 6%. In the years 2001-2002, Alan Greenspan, the chairman of the Fed decreased the interest rates to support the economy in reaction to the 9/11 and the Internet bubble. This ended up creating a lot of liquidity in the market. This led to the housing bubble which in turn created illiquidity and instability in the market, subsequently contributing to the Subprime Crisis. It is interesting to see how investors kept betting (even in 2008) that yields would increase on longer maturities despite the financial crisis. 
> The yield curve has flattened a lot for the past years , but slightly steepened for longer maturities. The Fed actually bought bonds with longer maturities in a hope to flatten longer maturity yield curve and boost the recovery of the economy. 

### 3-month and 10-year yields since 1999

```{r yield_curve_3, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "yield_curve3.png"), error = FALSE)
```


```{r}
Yields<- yield_curve %>%
  filter (maturity %in% c("3m","10y")) %>%
  filter (year(date) > 1998) 

ggplot() + 
geom_line(data=Yields, aes(x=date, y=value, color=duration))+ 
  theme_bw() + 
  theme(legend.title = element_blank())+
  theme(legend.position = "right")+
  scale_color_manual(values=c("cadetblue3","red"))+
  labs (title = "Yields on 3-month and 10-year US Treasury rates since 1999", y="%", x="",caption="Source: St. Louis Federal Reserve Economic Database (FRED)")


```
According to [Wikipedia's list of recession in the United States](https://en.wikipedia.org/wiki/List_of_recessions_in_the_United_States), since 1999 there have been two recession in the US: between Mar 2001–Nov 2001 and between Dec 2007–June 2009. Does the yield curve seem to flatten before these recessions? Can a yield curve flattening really mean a recession is coming in the US? Since 1999, when did short-term (3 months) yield more than longer term (10 years) debt?

> Theorically speaking, we know that a yield with a longer maturity should always be higher than a yield with a smaller maturity, as there is a risk premium over longer maturities - it should always be safer to buy right now than to buy in an uncertain futures. However, for short peiords of history, the yield curve inverted with means the 3-month yield was higher than the 10-year yield. When investors think it is riskier to buy today rather than in an uncertain future, it means the situation today is worse than it will be in the years to come. It is usually a signal of an impending crisis. And as perdiced, each inversion yield curve was followed by a financial crisis. In 2000 and 2006, the yield curve inverted. If you look at the central banks speeches following those inverted yield curves, they always try to reassure the markets... In 2019, there was another inversion of the yield curve. The market could not have predicted the covid crisis, but there were issues at the time that could also have led to a crisis. The very low rate environnement which ended up on equities sky valuations could have unveilled a financial bubble if rates had increased. Today, after 2 years of pandemic, we are faced with similar issues. The potential rise in yields poses a serious threat to the current excessive valuation of tech stocks, and could unveil a financial bubble. To quote Warren Buffet, “only when the tide goes out do you discover who's been swimming naked”. This will surely be one of the critical issues of the coming months. A yield curve flattening can really mean a recession is coming in the US. A research study from Deutsche Bank actually calculated the regression between inverted yield curves and recessions and found a very small p-value, which proved the statistical relationship of the variables. 

Besides calculating the spread (10year - 3months), there are a few things we need to do to produce our final plot

1. Setup data for US recessions 
1. Superimpose recessions as the grey areas in our plot
1. Plot the spread between 30 years and 3 months as a blue/red ribbon, based on whether the spread is positive (blue) or negative(red)


- For the first, the code below creates a dataframe with all US recessions since 1946

```{r setup_US-recessions, warning=FALSE}

# get US recession dates after 1946 from Wikipedia 
# https://en.wikipedia.org/wiki/List_of_recessions_in_the_United_States

recessions <- tibble(
  from = c("1948-11-01", "1953-07-01", "1957-08-01", "1960-04-01", "1969-12-01", "1973-11-01", "1980-01-01","1981-07-01", "1990-07-01", "2001-03-01", "2007-12-01","2020-02-01"),  
  to = c("1949-10-01", "1954-05-01", "1958-04-01", "1961-02-01", "1970-11-01", "1975-03-01", "1980-07-01", "1982-11-01", "1991-03-01", "2001-11-01", "2009-06-01", "2020-04-30") 
  )  %>% 
  mutate(From = ymd(from), 
         To=ymd(to),
         duration_days = To-From)


recessions
# Manipulate the data to find the difference of yield between 3m and 10 y maturity 
yield_curve_diff <- yield_curve%>%
  filter(maturity %in% c("3m","10y"))%>%
  pivot_wider(id_cols = c(date), names_from = maturity, values_from = value)%>%
  rename(y_10 = `10y`, m_3 = `3m`) %>% 
  mutate(diff = y_10 - m_3)%>%
  mutate(
  up = ifelse(y_10>m_3, diff, 0),down = ifelse(y_10<m_3, diff, 0)) %>% 
  mutate(true_false=ifelse(y_10>m_3, "Yes", "No")) 

```


```{r}
ggplot(yield_curve_diff) + 
  geom_rect(aes(x=date,xmin=as.Date("1960-04-01","%Y-%m-%d"),xmax=as.Date("1961-02-01","%Y-%m-%d"),ymin = -5,ymax = 5, group=date), fill = "grey69", alpha = 0.01)+
  geom_rect(aes(x=date,xmin=as.Date("1969-12-01","%Y-%m-%d"),xmax=as.Date("1970-11-01","%Y-%m-%d"),ymin = -5,ymax = 5, group=date), fill = "grey69", alpha = 0.01)+
  geom_rect(aes(x=date,xmin=as.Date("1973-11-01","%Y-%m-%d"),xmax=as.Date("1975-03-01","%Y-%m-%d"),ymin = -5,ymax = 5, group=date), fill = "grey69", alpha = 0.01)+
  geom_rect(aes(x=date,xmin=as.Date("1980-01-01","%Y-%m-%d"),xmax=as.Date("1980-07-01","%Y-%m-%d"),ymin = -5,ymax = 5, group=date), fill = "grey69", alpha = 0.01)+
  geom_rect(aes(x=date,xmin=as.Date("1981-07-01","%Y-%m-%d"),xmax=as.Date("1982-11-01","%Y-%m-%d"),ymin = -5,ymax = 5, group=date), fill = "grey69", alpha = 0.01)+
  geom_rect(aes(x=date,xmin=as.Date("1990-07-01","%Y-%m-%d"),xmax=as.Date("1991-03-01","%Y-%m-%d"),ymin = -5,ymax = 5, group=date), fill = "grey69", alpha = 0.01)+
  geom_rect(aes(x=date,xmin=as.Date("2001-03-01","%Y-%m-%d"),xmax=as.Date("2001-11-01","%Y-%m-%d"),ymin = -5,ymax = 5, group=date), fill = "grey69", alpha = 0.01)+
  geom_rect(aes(x=date,xmin=as.Date("2007-12-01","%Y-%m-%d"),xmax=as.Date("2009-06-01","%Y-%m-%d"),ymin = -5,ymax = 5, group=date), fill = "grey69", alpha = 0.01)+
  geom_rect(aes(x=date,xmin=as.Date("2020-02-01","%Y-%m-%d"),xmax=as.Date("2020-04-30","%Y-%m-%d"),ymin = -5,ymax = 5, group=date), fill = "grey69", alpha = 0.01)+
  geom_line(aes(x=date,y=diff))+
  geom_point(aes(x=date,y=diff), color = "transparent")+
  geom_rug(aes(x=date,color = true_false))+
  geom_ribbon(aes(x=date,ymin=down,ymax=0),fill="#CB454A",alpha=0.4)+
  geom_ribbon(aes(x=date,ymin=0,ymax=up),fill="steelblue3",alpha=0.4)+
  theme_bw()+
  theme(legend.position = "none")+
  labs(title= "Yield Curve Inversion: 10-year minus 3-month U.S. Treasury rates",
       subtitle="Difference in % points, monthly averages.
Shaded areas cirrespond to recessions",
       x="",
       y="Difference (10 year-3 month) yield in %",
       caption="Source.FRED.Federal Reserve Bank of St.Louis")
  
```
> As a conclusion, we see that inverted yield curves are a good inficator of potential recessions. As we mentionned earlier, theorically speaking, it does not make any sense to think it is more riskier to buy today than in a future we don't know. We see that an inversion was systematically followed by a crisis even when it happened in 2019, when it was impossible to predict what will happen. We need to stay tune and always follow the spread... 


# Challenge 2: GDP components over time and among countries

At the risk of oversimplifying things, the main components of gross domestic product, GDP are personal consumption (C), business investment (I), government spending (G) and net exports (exports - imports). You can read more about GDP and the different approaches in calculating at the [Wikipedia GDP page](https://en.wikipedia.org/wiki/Gross_domestic_product).

The GDP data we will look at is from the [United Nations' National Accounts Main Aggregates Database](https://unstats.un.org/unsd/snaama/Downloads), which contains estimates of total GDP and its components for all countries from 1970 to today. We will look at how GDP and its components have changed over time, and compare different countries and how much each component contributes to that country's GDP. The file we will work with is [GDP and its breakdown at constant 2010 prices in US Dollars](http://unstats.un.org/unsd/amaapi/api/file/6) and it has already been saved in the Data directory. Have a look at the Excel file to see how it is structured and organised

```{r read_GDP_data}

UN_GDP_data  <-  read_excel(here::here("data", "Download-GDPconstant-USD-countries.xls"), # Excel filename
                sheet="Download-GDPconstant-USD-countr", # Sheet name
                skip=2) # Number of rows to skip

```

The first thing you need to do is to tidy the data, as it is in wide format and you must make it into long, tidy format. Please express all figures in billions (divide values by `1e9`, or $10^9$), and you want to rename the indicators into something shorter.

```{r reshape_GDP_data}
library(reshape2)

tidy_GDP_data  <-  UN_GDP_data %>% 
  pivot_longer(cols=c("1970":"2017"), names_to="Year", values_to="value") %>% 
  mutate(value_b = value/10**9)
  
glimpse(tidy_GDP_data)

# Let us compare GDP components for these 3 countries
country_list <- c("United States", "India", "Germany")

tidy_GDP_data_1  <-  tidy_GDP_data %>% 
  filter(Country %in% country_list) %>% 
  filter(IndicatorName %in% c("Gross capital formation", "Exports of goods and services", "General government final consumption expenditure", "Household consumption expenditure (including Non-profit institutions serving households)", "Imports of goods and services")) %>% 
  mutate(IndicatorName_s = ifelse(IndicatorName == "Gross capital formation", "Gross capital formation",
                                  ifelse(IndicatorName == "Exports of goods and services", "Exports", 
                                         ifelse(IndicatorName == "Imports of goods and services", "Imports",
                                                ifelse(IndicatorName == "General government final consumption expenditure", "Govenment expenditure",
                                                       ifelse(IndicatorName == "Household consumption expenditure (including Non-profit institutions serving households)", "Household expenditure", "NA"))))))


```

First, can you produce this plot?

```{r gdp1, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "gdp1.png"), error = FALSE)
```
```{r}
ggplot(tidy_GDP_data_1, aes(x = Year, y = value_b, color = IndicatorName_s, group = IndicatorName_s))+
  geom_line()+
  facet_wrap(~Country)+
  labs(title = "GDP components over time",
       subtitle="In constant 2010 USD",
       x= "",
       y="Billion US$", 
       colour = "Components of GDP")+
  scale_x_discrete(breaks=seq(1900, 2010, 10))+ 
  theme_bw()+
    theme(axis.text.x =element_text(size=5))
  
```


Secondly, recall that GDP is the sum of Household Expenditure (Consumption *C*), Gross Capital Formation (business investment *I*), Government Expenditure (G) and Net Exports (exports - imports). Even though there is an indicator `Gross Domestic Product (GDP)` in your dataframe, I would like you to calculate it given its components discussed above.

```{r, cal_GDP_manually}
tidy_GDP_data_2  <-  tidy_GDP_data %>% 
  filter(Country %in% country_list) %>% 
  filter(IndicatorName %in% c("Gross capital formation", "Exports of goods and services", "General government final consumption expenditure", "Household consumption expenditure (including Non-profit institutions serving households)", "Imports of goods and services", "Gross Domestic Product (GDP)")) %>% 
  mutate(IndicatorName_s = ifelse(IndicatorName == "Gross capital formation", "Gross capital formation",
                                  ifelse(IndicatorName == "Exports of goods and services", "Exports", 
                                         ifelse(IndicatorName == "Imports of goods and services", "Imports",
                                                ifelse(IndicatorName == "General government final consumption expenditure", "Govenment expenditure",
                                                       ifelse(IndicatorName == "Household consumption expenditure (including Non-profit institutions serving households)", "Household expenditure", 
                                                              ifelse(IndicatorName == "Gross Domestic Product (GDP)", "GDP", "NA")))))))


GDP_compare <- tidy_GDP_data_2 %>% 
  select(!c(value, IndicatorName)) %>% 
  pivot_wider(id_cols = c(CountryID, Country, Year), names_from = IndicatorName_s, values_from = value_b) %>% 
  mutate(net_export = Exports-Imports,
         cal_GDP = `Household expenditure`+`Govenment expenditure`+`Gross capital formation`+net_export,
         diff = cal_GDP - GDP,
         percent_diff = (cal_GDP - GDP)/GDP,
         up = ifelse(diff > 0, diff, 0),
         down = ifelse(diff < 0, -diff, 0))
```

```{r}
# absolute difference between cal_GDP and GDP
ggplot(GDP_compare) +
  geom_line(aes(x = Year, y = GDP, group = Country), color = "red")+
  geom_line(aes(x = Year, y = cal_GDP, group = Country), color = "blue")+
  geom_ribbon(aes(x=Year,ymin=GDP,ymax=cal_GDP,group=Country),fill="#CB454A",alpha=0.4)+
  facet_wrap(~Country)+
  theme_bw()+
  labs(title = "Different in GDP and calculated GDP",
       subtitle="In constant 2010 USD",
       x= "",
       y="Billion US$")+
  scale_x_discrete(breaks=seq(1900, 2010, 10))

# percentage difference between cal_GDP and GDP
ggplot(GDP_compare)+
  geom_line(aes(x = Year, y = percent_diff, group = Country))+
  geom_hline(yintercept=0,linetype="solid",color="orange", size = 1)+
  facet_wrap(~Country)+
  theme_bw()+
  labs(title = "Percentage different in GDP and calculated GDP",
       subtitle="In constant 2010 USD",
       x= "",
       y="%")+
  scale_x_discrete(breaks=seq(1900, 2010, 10))
```

> What is the % difference between what you calculated as GDP and the GDP figure included in the dataframe?

> From the percentage difference in GDP and calculated GDP we can see that difference in the calculated GDP and reported GDP in the developed countries such as Germany and US, the difference is lower from 2000 onwards. We believe that this is from the better reporting and recored data that the countries collects more accurately. While developing countries such as India still have the percentage difference between calculated GDP and reported GDP up until now as we believe that there is still a problem in data collection. 

```{r gdp2, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "gdp2.png"), error = FALSE)
```
```{r}
proportion <- GDP_compare %>% 
  mutate(`Govenment expenditure` = `Govenment expenditure`/GDP,
         `Gross capital formation` = `Gross capital formation`/GDP,
         `Net Exports` = net_export/GDP,
         `Household expenditure` = `Household expenditure`/GDP) %>% 
  select(!c(Imports, Exports, diff, percent_diff, up, down, GDP, cal_GDP, net_export)) %>% 
  pivot_longer(cols=c("Govenment expenditure","Gross capital formation","Net Exports","Household expenditure"), names_to="Components", values_to="Proportion")

ggplot(proportion)+
  geom_line(aes(x = Year, y = Proportion, group = Components, color = Components), size=1.3)+
  facet_wrap(~Country)+
  theme_bw()+
  labs(title = "GDP and its breakdown at constant 2010 price in US Dollars",
       x= "",
       y="Proportion",
       caption="Source:United Nation, https://unstats.un.org/unsd/snaama/Downloads")+
  scale_x_discrete(breaks=seq(1900, 2010, 10))+
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))+
  theme(axis.text.x =element_text(size=5)) + 
  theme(legend.title = element_blank())

```

> What is this last chart telling you? Can you explain in a couple of paragraphs the different dynamic among these three countries?

Starting with big pictures, we can see that all countries have household expenditure as the highest proportion of its GDP components followed by gross capital formation, government expenditure and net export.

Germany which is the developed market started to see an increase in the proportion of its net export from year 2000. This may be because the country is exporting automobile globally. 

India has the recent development in its economic growth from 2000. Therefore, the gross capital formation (business investment) has been increasing from 2000 to present to support the economic growth as business expand its operation. As a result, it cause the decline in the household expenditure. 

US GDP component, we can see that the government expenditure and net export proportion to GDP has decreasing from 1970 to present.  


> If you want to, please change `country_list <- c("United States","India", "Germany")` to include your own country and compare it with any two other countries you like

```{r}
country_list <- c("Thailand", "China", "United Kingdom")
```

```{r}
tidy_GDP_data_1  <-  tidy_GDP_data %>% 
  filter(Country %in% country_list) %>% 
  filter(IndicatorName %in% c("Gross capital formation", "Exports of goods and services", "General government final consumption expenditure", "Household consumption expenditure (including Non-profit institutions serving households)", "Imports of goods and services")) %>% 
  mutate(IndicatorName_s = ifelse(IndicatorName == "Gross capital formation", "Gross capital formation",
                                  ifelse(IndicatorName == "Exports of goods and services", "Exports", 
                                         ifelse(IndicatorName == "Imports of goods and services", "Imports",
                                                ifelse(IndicatorName == "General government final consumption expenditure", "Govenment expenditure",
                                                       ifelse(IndicatorName == "Household consumption expenditure (including Non-profit institutions serving households)", "Household expenditure", "NA"))))))

ggplot(tidy_GDP_data_1, aes(x = Year, y = value_b, color = IndicatorName_s, group = IndicatorName_s))+
  geom_line()+
  facet_wrap(~Country)+
  theme_bw()+
  labs(title = "GDP components over time",
       subtitle="In constant 2010 USD",
       x= "",
       y="Billion US$", 
       colour = "Components of GDP")+
  scale_x_discrete(breaks=seq(1900, 2010, 10))
```

> This graph shows us the paradox of Chinese growth. Indeed, Chinese growth compared to growth in the UK, depends much more on gross capital formation than on household expenditure. For most countries (as we saw in previous graph), the primary source of GDP growth is household expenditure. Not in the case of China. Consumption in China therefore has space to grow. Thailand is a case in the middle of China and the UK but with much less expenses than the other two countries.

```{r}
tidy_GDP_data_2  <-  tidy_GDP_data %>% 
  filter(Country %in% country_list) %>% 
  filter(IndicatorName %in% c("Gross capital formation", "Exports of goods and services", "General government final consumption expenditure", "Household consumption expenditure (including Non-profit institutions serving households)", "Imports of goods and services", "Gross Domestic Product (GDP)")) %>% 
  mutate(IndicatorName_s = ifelse(IndicatorName == "Gross capital formation", "Gross capital formation",
                                  ifelse(IndicatorName == "Exports of goods and services", "Exports", 
                                         ifelse(IndicatorName == "Imports of goods and services", "Imports",
                                                ifelse(IndicatorName == "General government final consumption expenditure", "Govenment expenditure",
                                                       ifelse(IndicatorName == "Household consumption expenditure (including Non-profit institutions serving households)", "Household expenditure", 
                                                              ifelse(IndicatorName == "Gross Domestic Product (GDP)", "GDP", "NA")))))))


GDP_compare <- tidy_GDP_data_2 %>% 
  select(!c(value, IndicatorName)) %>% 
  pivot_wider(id_cols = c(CountryID, Country, Year), names_from = IndicatorName_s, values_from = value_b) %>% 
  mutate(net_export = Exports-Imports,
         cal_GDP = `Household expenditure`+`Govenment expenditure`+`Gross capital formation`+net_export,
         diff = cal_GDP - GDP,
         percent_diff = (cal_GDP - GDP)/GDP,
         up = ifelse(diff > 0, diff, 0),
         down = ifelse(diff < 0, -diff, 0))
```

```{r}
# absolute difference between cal_GDP and GDP
ggplot(GDP_compare) +
  geom_line(aes(x = Year, y = GDP, group = Country), color = "red")+
  geom_line(aes(x = Year, y = cal_GDP, group = Country), color = "blue")+
  geom_ribbon(aes(x=Year,ymin=GDP,ymax=cal_GDP,group=Country),fill="#CB454A",alpha=0.4)+
  facet_wrap(~Country)+
  theme_bw()+
  labs(title = "Different in GDP and calculated GDP",
       subtitle="In constant 2010 USD",
       x= "",
       y="Billion US$")+
  scale_x_discrete(breaks=seq(1900, 2010, 10))

# percentage difference between cal_GDP and GDP
ggplot(GDP_compare)+
  geom_line(aes(x = Year, y = percent_diff, group = Country))+
  geom_hline(yintercept=0,linetype="solid",color="orange", size = 1)+
  facet_wrap(~Country)+
  theme_bw()+
  labs(title = "Percentage different in GDP and calculated GDP",
       subtitle="In constant 2010 USD",
       x= "",
       y="%")+
  scale_x_discrete(breaks=seq(1900, 2010, 10))
```

```{r}
proportion <- GDP_compare %>% 
  mutate(`Govenment expenditure` = `Govenment expenditure`/GDP,
         `Gross capital formation` = `Gross capital formation`/GDP,
         `Net Exports` = net_export/GDP,
         `Household expenditure` = `Household expenditure`/GDP) %>% 
  select(!c(Imports, Exports, diff, percent_diff, up, down, GDP, cal_GDP, net_export)) %>% 
  pivot_longer(cols=c("Govenment expenditure","Gross capital formation","Net Exports","Household expenditure"), names_to="Components", values_to="Proportion")

ggplot(proportion)+
  geom_line(aes(x = Year, y = Proportion, group = Components, color = Components), size=1.3)+
  facet_wrap(~Country)+
  theme_bw()+
  labs(title = "Percentage different in GDP and calculated GDP",
       subtitle="In constant 2010 USD",
       x= "",
       y="Proportion")+
  scale_x_discrete(breaks=seq(1900, 2010, 10))
```

# Deliverables

There is a lot of explanatory text, comments, etc. You do not need these, so delete them and produce a stand-alone document that you could share with someone. Knit the edited and completed R Markdown file as an HTML document (use the "Knit" button at the top of the script editor window) and upload it to Canvas.

# Details

- Who did you collaborate with: TYPE NAMES HERE
- Approximately how much time did you spend on this problem set: ANSWER HERE
- What, if anything, gave you the most trouble: ANSWER HERE


**Please seek out help when you need it,** and remember the [15-minute rule](https://mfa2022.netlify.app/syllabus/#the-15-minute-rule){target=_blank}. You know enough R (and have enough examples of code from class and your readings) to be able to do this. If you get stuck, ask for help from others, post a question on Slack-- and remember that I am here to help too!  

> As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else? 


# Rubric

Check minus (1/5): Displays minimal effort. Doesn't complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn't use plots appropriate for the variables being analyzed. 

Check (3/5): Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output). 

Check plus (5/5): Finished all components of the assignment correctly and addressed both challenges. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you've written additional text to describe how you interpret the output.



